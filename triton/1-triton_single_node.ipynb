{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single node triton server with load test\n",
    "1. Loading to and from minio workflow (multiple models)\n",
    "2. Getting models from [timm](https://timm.fast.ai/)\n",
    "3. Making the node\n",
    "4. Load and unload models operations\n",
    "5. Monitoring\n",
    "6. Naive load test\n",
    "7. TODO Add language models from Huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(timm.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('sudo umount -l ~/my_mounting_point')\n",
    "os.system('cc-cloudfuse mount ~/my_mounting_point')\n",
    "\n",
    "data_folder_path = '/home/cc/my_mounting_point/datasets'\n",
    "dataset_folder_path = os.path.join(\n",
    "    data_folder_path, 'ILSVRC/Data/DET/test'\n",
    ")\n",
    "classes_file_path = os.path.join(\n",
    "    data_folder_path, 'imagenet_classes.txt'\n",
    ")\n",
    "\n",
    "image_names = os.listdir(dataset_folder_path)\n",
    "image_names.sort()\n",
    "with open(classes_file_path) as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "def image_loader(folder_path, image_name):\n",
    "    image = Image.open(\n",
    "        os.path.join(folder_path, image_name))\n",
    "    # if there was a need to filter out only color images\n",
    "    # if image.mode == 'RGB':\n",
    "    #     pass\n",
    "    return image\n",
    "num_loaded_images = 20\n",
    "images = {\n",
    "    image_name: image_loader(\n",
    "        dataset_folder_path, image_name) for image_name in image_names[\n",
    "            :num_loaded_images]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and transform model\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")])\n",
    "\n",
    "batch = torch.stack(list(map(lambda a: transform(a), list(images.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lion', 'studio_couch', 'harp', 'goldfish', 'snowplow',\n",
       "       'pomegranate', 'alligator_lizard', 'stethoscope', 'banjo', 'junco',\n",
       "       'shoe_shop', 'albatross', 'notebook', 'lesser_panda', 'microphone',\n",
       "       'football_helmet', 'damselfly', 'barrel', 'harp', 'mortarboard'],\n",
       "      dtype='<U30')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and predict with the timm model\n",
    "model_name = 'resnet50'\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "model.eval()\n",
    "torch_output = model(batch)\n",
    "torch_output = torch.nn.functional.softmax(torch_output, dim=1) * 100\n",
    "torch_output = torch_output.detach().numpy()\n",
    "torch_output = torch_output.argmax(axis=1)\n",
    "torch_class = np.array(classes)[torch_output]\n",
    "torch_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the onnx model\n",
    "import torch.onnx\n",
    "\n",
    "if 'models' not in os.listdir(\"./\"):\n",
    "    os.mkdir('models')\n",
    "# Standard ImageNet input - 3 channels, 224x224,\n",
    "# values don't matter as we care about network structure.\n",
    "# But they can also be real inputs.\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "# Invoke export\n",
    "torch.onnx.export(\n",
    "    model, dummy_input,\n",
    "    f\"./models/{model_name}.onnx\",\n",
    "    input_names = ['input'],\n",
    "    output_names = ['output'],\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                  'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4050229/3273098817.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  onnx_output = torch.nn.functional.softmax(torch.tensor(onnx_output), dim=1)[0] * 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['lion', 'studio_couch', 'slot', 'goldfish', 'snowplow',\n",
       "       'pomegranate', 'alligator_lizard', 'seat_belt', 'hatchet', 'junco',\n",
       "       'shoe_shop', 'albatross', 'bookshop', 'lesser_panda', 'marimba',\n",
       "       'football_helmet', 'damselfly', 'horse_cart', 'harp',\n",
       "       'mortarboard'], dtype='<U30')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use onnx model\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "onnx_model = onnx.load(f\"./models/{model_name}.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(f\"{model_name}.onnx\", providers=['CPUExecutionProvider'])\n",
    "onnx_output = ort_session.run(None, {'input': batch.numpy()})\n",
    "onnx_output = torch.nn.functional.softmax(torch.tensor(onnx_output), dim=1)[0] * 100\n",
    "onnx_output = onnx_output.detach().numpy()\n",
    "onnx_output = onnx_output.argmax(axis=1)\n",
    "onnx_class = np.array(classes)[onnx_output]\n",
    "onnx_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/triton/1-triton_single_node.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/triton/1-triton_single_node.ipynb#ch0000008vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# TODO find out why slightly different\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/triton/1-triton_single_node.ipynb#ch0000008vscode-remote?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mall(onnx_output \u001b[39m==\u001b[39m torch_output)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/triton/1-triton_single_node.ipynb#ch0000008vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(onnx_class)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO find out why slightly different\n",
    "\n",
    "assert np.all(onnx_output == torch_output)\n",
    "print(onnx_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying on Onnx Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.05-py3: Pulling from nvidia/tritonserver\n",
      "Digest: sha256:a85daa2907f46e70b3782818a0331df62d9b4e0b1f15f1530b2a52c8c782d46d\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "\n",
      "=============================\n",
      "== Triton Inference Server ==\n",
      "=============================\n",
      "\n",
      "NVIDIA Release 22.05 (build 38317651)\n",
      "Triton Server Version 2.22.0\n",
      "\n",
      "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n",
      "   Use the NVIDIA Container Toolkit to start this container with GPU support; see\n",
      "   https://docs.nvidia.com/datacenter/cloud-native/ .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 18:56:30.001279 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\n",
      "I0624 18:56:30.001530 1 cuda_memory_manager.cc:115] CUDA memory pool disabled\n",
      "E0624 18:56:30.002653 1 model_repository_manager.cc:2063] Poll failed for model directory 'resnet': model output has different size for dims and reshape for resnet\n",
      "I0624 18:56:30.002702 1 server.cc:556] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0624 18:56:30.002720 1 server.cc:583] \n",
      "+---------+------+--------+\n",
      "| Backend | Path | Config |\n",
      "+---------+------+--------+\n",
      "+---------+------+--------+\n",
      "\n",
      "I0624 18:56:30.002737 1 server.cc:626] \n",
      "+-------+---------+--------+\n",
      "| Model | Version | Status |\n",
      "+-------+---------+--------+\n",
      "+-------+---------+--------+\n",
      "\n",
      "I0624 18:56:30.002948 1 tritonserver.cc:2138] \n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Option                           | Value                                                                                                                                                                                        |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| server_id                        | triton                                                                                                                                                                                       |\n",
      "| server_version                   | 2.22.0                                                                                                                                                                                       |\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\n",
      "| model_repository_path[0]         | /models                                                                                                                                                                                      |\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                    |\n",
      "| strict_model_config              | 1                                                                                                                                                                                            |\n",
      "| rate_limit                       | OFF                                                                                                                                                                                          |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                            |\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                          |\n",
      "| strict_readiness                 | 1                                                                                                                                                                                            |\n",
      "| exit_timeout                     | 30                                                                                                                                                                                           |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0624 18:56:30.002965 1 server.cc:257] Waiting for in-flight requests to complete.\n",
      "I0624 18:56:30.002973 1 server.cc:273] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I0624 18:56:30.002979 1 server.cc:288] All models are stopped, unloading models\n",
      "I0624 18:56:30.002984 1 server.cc:295] Timeout 30: Found 0 live models and 0 in-flight non-inference requests\n",
      "error: creating server: Internal - failed to load all models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VERSION='22.05'\n",
    "os.system(f\"docker pull nvcr.io/nvidia/tritonserver:{VERSION}-py3\")\n",
    "# add --gpus=<number of gpus> on gpu machines\n",
    "os.system(\"docker run --rm -p8000:8000 -p8001:8001 -p8002:8002\"\n",
    "          f\" -v {os.getcwd()}/models:/models \"\n",
    "          f\"nvcr.io/nvidia/tritonserver:{VERSION}-py3 tritonserver --model-repository=/models\")\n",
    "# print(\"docker run --rm -p8000:8000 -p8001:8001 -p8002:8002\"\n",
    "#           f\" -v {os.getcwd()}/models:/models \"\n",
    "#           f\"nvcr.io/nvidia/tritonserver:{VERSION}-py3 tritonserver --model-repository=/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use python to sent requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a bunch of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Switching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model load and offload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'onnx_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/triton/1-triton_single_node.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/triton/1-triton_single_node.ipynb#ch0000022vscode-remote?line=0'>1</a>\u001b[0m onnx_model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'onnx_model' is not defined"
     ]
    }
   ],
   "source": [
    "onnx_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seldon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add model switching etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('central')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2465c4f56298bc06dbdad3e7519856d346ec0e9edf6ba2c905f0af711583810e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
