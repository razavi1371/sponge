audio-qa:
  audio:
    # TODO script
    # merge accuracy comparison scripts of
    # https://huggingface.co/facebook/s2t-small-librispeech-asr family
    # and
    # https://huggingface.co/facebook/wav2vec2-base-960h
    facebook/s2t-small-librispeech-asr: 80
    facebook/s2t-medium-librispeech-asr: 82
    facebook/s2t-large-librispeech-asr: 83
    facebook/wav2vec2-base-960h: 85
    facebook/wav2vec2-large-960h: 86
  nlp-qa:
    # TODO find also the dataset (both train-test) if no dataset then script like audio
    deepset/roberta-base-squad2: 82.9125  # F1 score, https://huggingface.co/deepset/roberta-base-squad2#performance
    deepset/xlm-roberta-large-squad2: 83.7925  # F1 score, https://huggingface.co/deepset/xlm-roberta-large-squad2#performance
    distilbert-base-cased-distilled-squad: 87.1  # F1 score, https://huggingface.co/distilbert-base-cased-distilled-squad#evaluation
    deepset/xlm-roberta-base-squad2: 77.141  # F1 score, https://huggingface.co/deepset/xlm-roberta-base-squad2#performance
audio-sent:
  audio:
    # TODO script
    facebook/s2t-small-librispeech-asr: 80
    facebook/s2t-medium-librispeech-asr: 82
    facebook/s2t-large-librispeech-asr: 83
    facebook/wav2vec2-base-960h: 85
    facebook/wav2vec2-large-960h: 86
  nlp-sent:
  # sentimment-analysis
  # TODO find also the dataset (both train-test) if no dataset then script like audio
    huggingface/distilbert-base-uncased-finetuned-mnli: 82.2  # https://huggingface.co/distilbert-base-uncased
    huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli: nill
    distilbert-base-uncased-finetuned-sst-2-english: 91.3  # https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english
    Souvikcmsa/BERT_sentiment_analysis: 0.799017824663514  # https://huggingface.co/Souvikcmsa/BERT_sentiment_analysis
    Souvikcmsa/SentimentAnalysisDistillBERT: 0.7962895598399418  # https://huggingface.co/Souvikcmsa/SentimentAnalysisDistillBERT
    Souvikcmsa/Roberta_Sentiment_Analysis: 0.8302828618968386  # https://huggingface.co/Souvikcmsa/Roberta_Sentiment_Analysis
mock:
  node-1:
    1: 100
    2: 100
  node-2:
    1: 100
    2: 100
nlp:
# language identification
  nlp-li:
    dinalzein/xlm-roberta-base-finetuned-language-identification: 0.9959  # Accuracy, https://huggingface.co/dinalzein/xlm-roberta-base-finetuned-language-identification
  # text translation
  nlp-trans:
    # TODO find also the dataset (both train-test) if no dataset then script like audio
    Helsinki-NLP/opus-mt-fr-en: 0.720  # chrF2 score, https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.eval.txt
    Helsinki-NLP/opus-mt-tc-big-fr-en: 0.74090  # chrF2 score, https://object.pouta.csc.fi/Tatoeba-MT-models/fra-eng/opusTCv20210807+bt_transformer-big_2022-03-09.eval.txt
  # text summerization
  nlp-sum:
    # TODO find also the dataset (both train-test) if no dataset then script like audio
    # ROUGE-L metric for sshleifer versions https://huggingface.co/sshleifer/distilbart-xsum-12-1#metrics-for-distilbart-models
    sshleifer/distilbart-cnn-12-6: 30.59
    sshleifer/distilbart-xsum-1-1: nill
    sshleifer/distill-pegasus-cnn-16-4: nill
    sshleifer/distill-pegasus-xsum-16-4: nill
    sshleifer/distilbart-xsum-12-3: 36.39
    sshleifer/distilbart-xsum-6-6: 35.73
    sshleifer/pegasus-cnn-ft-v2: nill
    sshleifer/distilbart-cnn-6-6: 29.70
    sshleifer/distilbart-xsum-12-6: 36.99
    sshleifer/distilbart-cnn-12-3: 30.00
    sshleifer/distilbart-xsum-12-1: 33.37
    sshleifer/distilbart-xsum-9-6: 36.61
    sshleifer/distill-pegasus-xsum-16-8: nill
    facebook/bart-large-cnn: 30.619  # ROUGE-L, https://huggingface.co/facebook/bart-large-cnn, https://paperswithcode.com/sota/summarization-on-cnn-dailymail
    google/roberta2roberta_L-24_bbc: nill
    google/pegasus-cnn_dailymail: nill
    google/roberta2roberta_L-24_cnn_daily_mail: nill
    google/pegasus-large: nill
sum-qa:
  nlp-sum:
    # TODO find also the dataset (both train-test) if no dataset then script like audio
    # ROUGE-L metric for sshleifer versions https://huggingface.co/sshleifer/distilbart-xsum-12-1#metrics-for-distilbart-models
    sshleifer/distilbart-cnn-12-6: 30.59
    sshleifer/distilbart-xsum-1-1: nill
    sshleifer/distill-pegasus-cnn-16-4: nill
    sshleifer/distill-pegasus-xsum-16-4: nill
    sshleifer/distilbart-xsum-12-3: 36.39
    sshleifer/distilbart-xsum-6-6: 35.73
    sshleifer/pegasus-cnn-ft-v2: nill
    sshleifer/distilbart-cnn-6-6: 29.70
    sshleifer/distilbart-xsum-12-6: 36.99
    sshleifer/distilbart-cnn-12-3: 30.00
    sshleifer/distilbart-xsum-12-1: 33.37
    sshleifer/distilbart-xsum-9-6: 36.61
    sshleifer/distill-pegasus-xsum-16-8: nill
    facebook/bart-large-cnn: 30.619  # ROUGE-L, https://huggingface.co/facebook/bart-large-cnn, https://paperswithcode.com/sota/summarization-on-cnn-dailymail
    google/roberta2roberta_L-24_bbc: nill
    google/pegasus-cnn_dailymail: nill
    google/roberta2roberta_L-24_cnn_daily_mail: nill
    google/pegasus-large: nill
  nlp-qa:
    deepset/roberta-base-squad2: 82.9125  # F1 score, https://huggingface.co/deepset/roberta-base-squad2#performance
    deepset/xlm-roberta-large-squad2: 83.7925  # F1 score, https://huggingface.co/deepset/xlm-roberta-large-squad2#performance
    distilbert-base-cased-distilled-squad: 87.1  # F1 score, https://huggingface.co/distilbert-base-cased-distilled-squad#evaluation
    deepset/xlm-roberta-base-squad2: 77.141  # F1 score, https://huggingface.co/deepset/xlm-roberta-base-squad2#performance
video:
  crop:
    # source https://github.com/ultralytics/yolov5
    yolov5n: 45.7
    yolov5s: 56.8
    yolov5m: 64.1
    yolov5l: 67.3
    yolov5x: 68.9
    # https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights
  classification:
    resnet18: 69.75
    resnet34: 73.31
    resnet50: 76.13
    resnet101: 77.37
    resnet152: 78.31