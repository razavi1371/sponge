# ----------- pipeline configs -------------
# pipeline informat
pipeline_folder_name: audio-qa
pipeline_name: audio-qa
timeout: 1
pipeline_folder_name: audio-qa
mode: exponential
nodes:
  - node_name: audio
    data_type: audio
    model_variants: facebook-s2t-small-librispeech-asr
    max_batch_size: '1'
    max_batch_time: '1'
    cpu_request: '1'
    memory_request: 4Gi
    replicas: 27
    use_threading: 'True'
    num_interop_threads: '1'
    num_threads: '1'
  - node_name: nlp-qa
    data_type: text
    model_variants: deepset-roberta-base-squad2
    max_batch_size: '1'
    max_batch_time: '1'
    cpu_request: '1'
    memory_request: 4Gi
    replicas: 6
    use_threading: 'True'
    num_interop_threads: '1'
    num_threads: '1'

# ----------- experiment configs -------------

benchmark_duration: 1
adaptation_interval: 10
# workload_type: static
# workload_config:
#   loads_to_test: 10
#   load_duration: 1800
workload_type: twitter
workload_config:
  - start: '1301160'
    end: '1302360'
    # end: '1863200'
    # end: '1862200'
    # end: '1861960'
    # end: '1862500'
    # end: '1863700'
    # end: '1865100'
    damping_factor: 8 # [int | null]

# ----------- optimizer configs -------------

metaseries: 51
series: 25
# pipeline_name: video
metadata: 'bursty - rim-high - cpu type: compute_cascadelake_r_ib - redo of metaseries bursty on chameleon to check latencies'
number_tasks: 2
profiling_series:
  - 85
  - 86
model_name:
  - audio # be consistent with values in pipelines
  - nlp-qa # be consistent with values in pipelines
task_name: 
  - audio
  - nlp-qa
initial_active_model:
  - facebook-s2t-small-librispeech-asr
  - deepset-roberta-base-squad2
initial_cpu_allocation:
  - 1
  - 1
initial_replica:
  - 27
  - 6
initial_batch:
  - 1
  - 1
alpha: 10
beta: 0.1
gamma: 0.000001
scaling_cap: 100 # maximum possible scaling per nodes
batching_cap: 1 # maximum possible batching per nodes
num_state_limit: 1 # number of states
threshold: 1.2 # RPS threshold for finding the base allocation
optimization_method: gurobi # options: [brute-force | gurobi]
allocation_mode: base # options: [fix | base | variable]
sla_factor: 5
normalize_accuracy: true
only_measured_profiles: true
profiling_load: 20
baseline_mode: switch # [switch | scale | null]
accuracy_method: multiply # [average | sum | multiply]
lowest_model_accuracy: 0.05

# ----------- load predictor configs -------------

predictor_type: 'lstm' # [reactive | avg | lstm | max]
monitoring_duration: 2 # in minutes
backup_predictor_type: 'max' # [reactive | avg | lstm | max]
backup_predictor_duration: 2 # in minutes

# ----------- queueing-option -------------

central_queue: true

# ----------- distruption time -------------

distrpution_time: 30

# ----------- distruption time -------------
# debug mode with complete lgos of containers
debug_mode: false

# ----------- simulation mode -------------
simulation_mode: false

# ----------- percent added to the predictor load -------------
predictor_margin: 20

# ----------- drop interval -------------
drop_limit: 20

# ----------- do a 10 second warm up before starting the experiment -------------
warm_up: false

# ----------- teleport mode (only available in real-world experiments) -------------
teleport_mode: false 
teleport_interval: 0

# ----------- reference latency and throughput -------------
reference_latency: 'p99' # p99 | avg
reference_throughput: 'max' # max | p99 | avg
latency_margin: 100
throughput_margin: -50

# ----------- enable in container logging -------------
logs_enabled: false

# ----------- whether to read models from storage or not - should be defined per node -------------
from_storage:
  - true
  - true

