# ----------- pipeline configs -------------
# pipeline informat
pipeline_folder_name: nlp
pipeline_name: nlp
timeout: 1
pipeline_folder_name: nlp
mode: exponential
nodes:
  - node_name: nlp-li
    data_type: text
    model_variants: dinalzein-xlm-roberta-base-finetuned-language-identification
    max_batch_size: '1'
    max_batch_time: '1'
    cpu_request: '1'
    memory_request: 4Gi
    replicas: 2
    use_threading: 'True'
    num_interop_threads: '1'
    num_threads: '1'
  - node_name: nlp-trans
    data_type: text
    model_variants: Helsinki-NLP-opus-mt-fr-en
    max_batch_size: '1'
    max_batch_time: '1'
    cpu_request: '2'
    memory_request: 4Gi
    replicas: 8
    use_threading: 'True'
    num_interop_threads: '1'
    num_threads: '1'
  - node_name: nlp-sum
    data_type: text
    model_variants: sshleifer-distilbart-xsum-1-1
    max_batch_size: '1'
    max_batch_time: '1'
    cpu_request: '2'
    memory_request: 4Gi
    replicas: 3
    use_threading: 'True'
    num_interop_threads: '1'
    num_threads: '1'

# ----------- experiment configs -------------

benchmark_duration: 1
adaptation_interval: 10
# workload_type: static
# workload_config:
#   loads_to_test: 10
#   load_duration: 1800
workload_type: twitter
workload_config:
  - start: '1768800'
    end: '1769400'
    damping_factor: 8 # [int | null]
  - start: '1768800'
    end: '1769400'
    damping_factor: 8 # [int | null]

# ----------- optimizer configs -------------

metaseries: 21
series: 94
# pipeline_name: video
metadata: 'steady high - rim-low - cpu type: compute_cascadelake_r_ib'
number_tasks: 3
profiling_series:
  - 98
  - 99
  - 97
model_name:
  - nlp-li
  - nlp-trans
  - nlp-sum
task_name: 
  - nlp-li
  - nlp-trans
  - nlp-sum
initial_active_model:
  - dinalzein-xlm-roberta-base-finetuned-language-identification
  - Helsinki-NLP-opus-mt-fr-en
  - sshleifer-distilbart-xsum-1-1
initial_cpu_allocation:
  - 1
  - 2
  - 2
initial_replica:
  - 2
  - 8
  - 3
initial_batch:
  - 1
  - 1
  - 1
alpha: 10
beta: 0.5
gamma: 0.000001
scaling_cap: 100 # maximum possible scaling per nodes
batching_cap: 2 # maximum possible batching per nodes
num_state_limit: 1 # number of states
threshold: 1 # RPS threshold for finding the base allocation
optimization_method: gurobi # options: [brute-force | gurobi]
allocation_mode: base # options: [fix | base | variable]
sla_factor: 5
normalize_accuracy: true
only_measured_profiles: true
profiling_load:
  - 20
  - 10
  - 10
baseline_mode: switch # [switch | scale | null]
accuracy_method: sum # [average | sum | multiply]

# ----------- load predictor configs -------------

predictor_type: 'lstm' # [reactive | avg | arima | lstm | max]
monitoring_duration: 2 # in minutes
backup_predictor_type: 'max' # [reactive | avg | lstm | max]
backup_predictor_duration: 2 # in minutes


# ----------- queueing-option -------------

central_queue: true

# ----------- distruption time -------------

distrpution_time: 30

# ----------- distruption time -------------
# debug mode with complete lgos of containers
debug_mode:  false

# ----------- simulation mode -------------
simulation_mode: false

# ----------- percent added to the predictor load -------------
predictor_margin: 0

# ----------- drop interval -------------
drop_limit: 20

# ----------- do a 10 second warm up before starting the experiment -------------
warm_up: false

# ----------- teleport mode (only available in real-world experiments) -------------
teleport_mode: false 
teleport_interval: 0

# ----------- teleport mode (only available in real-world experiments) -------------
teleport_mode: false 
teleport_interval: 0

# ----------- reference latency and throughput -------------
reference_latency: 'p99' # p99 | avg
reference_throughput: 'max' # max | p99 | avg
latency_margin: 0
throughput_margin: -50

# ----------- enable in container logging -------------
logs_enabled: false

# ----------- whether to read models from storage or not - should be defined per node -------------
from_storage:
  - true
  - true
  - true