# ----------- pipeline configs -------------
# pipeline informat
pipeline_folder_name: video
pipeline_name: video
timeout: 1
pipeline_folder_name: video
mode: exponential
nodes:
  - node_name: yolo
    data_type: image
    model_variants: yolov5n
    max_batch_size: '1'
    max_batch_time: '1'
    cpu_request: '1'
    memory_request: 4Gi
    replicas: 3
    use_threading: 'True'
    num_interop_threads: '1'
    num_threads: '1'
  - node_name: resnet-human
    data_type: text
    model_variants: resnet18
    max_batch_size: '1'
    max_batch_time: '1'
    cpu_request: '1'
    memory_request: 4Gi
    replicas: 3
    use_threading: 'True'
    num_interop_threads: '1'
    num_threads: '1'

# ----------- experiment configs -------------

benchmark_duration: 1
adaptation_interval: 10
# workload_type: static
# workload_config:
#   loads_to_test: 10
#   load_duration: 1800
workload_type: twitter
workload_config:
  - start: '1301400'
    end: '1302000'
    damping_factor: 8 # [int | null]
  - start: '1308600'
    end: '1309200'
    damping_factor: 8 # [int | null]

# ----------- optimizer configs -------------

metaseries: 18
series: 20
# pipeline_name: video
metadata: 'fluctuating - rim-high - cpu type: compute_cascadelake_r_ib'
number_tasks: 2
profiling_series:
  - 71
  - 72
model_name:
  - yolo
  - resnet-human
task_name: 
  - crop
  - classification
initial_active_model:
  - yolov5s
  - resnet101
initial_cpu_allocation:
  - 1
  - 1
initial_replica:
  - 3
  - 3
initial_batch:
  - 1
  - 1
alpha: 2
beta: 1
gamma: 0.000001
scaling_cap: 100 # maximum possible scaling per nodes
batching_cap: 8 # maximum possible batching per nodes
num_state_limit: 1 # number of states
threshold: 4 # RPS threshold for finding the base allocation
optimization_method: gurobi # options: [brute-force | gurobi]
allocation_mode: base # options: [fix | base | variable]
sla_factor: 5
normalize_accuracy: true
only_measured_profiles: true
profiling_load: 20
baseline_mode: switch # [switch | scale | null]
accuracy_method: sum # [average | sum | multiply]

# ----------- load predictor configs -------------

predictor_type: 'lstm' # [reactive | avg | arima | lstm | max]
monitoring_duration: 2 # in minutes
backup_predictor_type: 'max' # [reactive | avg | lstm | max]
backup_predictor_duration: 2 # in minutes

# ----------- queueing-option -------------

central_queue: true

# ----------- distruption time -------------

distrpution_time: 30

# ----------- distruption time -------------
# debug mode with complete lgos of containers
debug_mode:  false

# ----------- simulation mode -------------
simulation_mode: false

# ----------- percent added to the predictor load -------------
predictor_margin: 0

# ----------- drop interval -------------
drop_limit: 10

# ----------- do a 10 second warm up before starting the experiment -------------
warm_up: false

# ----------- teleport mode (only available in real-world experiments) -------------
teleport_mode: false 
teleport_interval: 0

# ----------- reference latency and throughput -------------
reference_latency: 'p99' # p99 | avg
reference_throughput: 'max' # max | p99 | avg
latency_margin: 0
throughput_margin: 0

# ----------- enable in container logging -------------
logs_enabled: false

# ----------- whether to read models from storage or not - should be defined per node -------------
from_storage:
  - true
  - true
