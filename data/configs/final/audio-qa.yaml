# ----------- experiment series -------------
series: 100

# ----------- pipeline configs -------------
# pipeline informat
pipeline_folder_name: audio-qa
pipeline_name: audio-qa
timeout: 1
pipeline_folder_name: audio-qa
mode: exponential
nodes:
  - node_name: audio
    data_type: audio
    model_variants: facebook-s2t-small-librispeech-asr
    max_batch_size: '1'
    max_batch_time: '1'
    cpu_request: '2'
    memory_request: 2Gi
    replicas: 1
    use_threading: 'True'
    num_interop_threads: '1'
    num_threads: '1'
  - node_name: nlp-qa
    data_type: text
    model_variants: deepset-roberta-base-squad2
    max_batch_size: '1'
    max_batch_time: '1'
    cpu_request: '2'
    memory_request: 2Gi
    replicas: 1
    use_threading: 'True'
    num_interop_threads: '1'
    num_threads: '1'

# ----------- experiment configs -------------

benchmark_duration: 1
adaptation_interval: 20
# workload_type: static
# workload_config:
#   loads_to_test: 10
#   load_duration: 1800
workload_type: twitter
workload_config:
  - start: '1863050'
    end: '1863200'
    # end: '1863200'
    # end: '1862200'
    # end: '1861960'
    # end: '1862500'
    # end: '1863700'
    # end: '1865100'
    damping_factor: 8 # [int | null]

# ----------- optimizer configs -------------
pipeline_name: audio-qa
metadata: 'first test of audio pipeline'
number_tasks: 2
profiling_series:
  - 85
  - 86
model_name:
  - audio # be consistent with values in pipelines
  - nlp-qa # be consistent with values in pipelines
task_name: 
  - audio
  - nlp-qa
initial_active_model:
  - facebook-s2t-small-librispeech-asr
  - distilbert-base-cased-distilled-squad
initial_cpu_allocation:
  - 1
  - 1
initial_replica:
  - 1
  - 1
initial_batch:
  - 1
  - 1
alpha: 0
beta: 1
gamma: 0
scaling_cap: 100 # maximum possible scaling per nodes
batching_cap: 64 # maximum possible batching per nodes
num_state_limit: 1 # number of states
threshold: 4 # RPS threshold for finding the base allocation
optimization_method: gurobi # options: [brute-force | gurobi]
allocation_mode: base # options: [fix | base | variable]
sla_factor: 5
normalize_accuracy: true
only_measured_profiles: false
profiling_load: 20
baseline_mode: scale # [switch | scale | null]
accuracy_method: sum # [average | sum | multiply]

# ----------- load predictor configs -------------

predictor_type: 'max' # [reactive | avg | lstm | max]
monitoring_duration: 2 # in minutes
backup_predictor_type: 'max' # [reactive | avg | max]
backup_predictor_duration: 2 # in minutes

# ----------- queueing-option -------------

central_queue: true

# ----------- distruption time -------------

distrpution_time: 30

# ----------- distruption time -------------
# debug mode with complete lgos of containers
debug_mode:  false

# ----------- simulation mode -------------
simulation_mode: false

# ----------- percent added to the predictor load -------------
predictor_margin: 0

# ----------- do a 10 second warm up before starting the experiment -------------
warm_up: false

# ----------- teleport mode (only available in real-world experiments) -------------
teleport_mode: false
teleport_interval: 10

# ----------- reference latency and throughput -------------
reference_latency: 'p99' # p99 | avg
reference_throughput: 'max' # max | p99 | avg
latency_margin: 0
throughput_margin: 0