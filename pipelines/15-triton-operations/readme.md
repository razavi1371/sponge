# Triton
## Usage

Triton provides an inference server which can use locally or by cc.
to start triton :
```
docker run --gpus=1 --rm --net=host -v repo:/models nvcr.io/nvidia/tritonserver:22.05-py3 tritonserver --model-repository=/models
```
this command bring us triton server from ngc container.

actually, trition use model repository which you can see in command that is address of our models.

you can switch gpu inference mode to cpu by the command you can see.

## Model management

triton give us apis for model management which we can access to them by rest apis or grpc. there are three ways to control our models:

- None : In this mode, if we changes model repo runtime, changes will be ignored. all models load in start time. use this mode by ``` --model-control-mode=none ```
- EXPLICIT : if we want use this mode, we should explicitly tell trition by flag ```--load-model```. use this mode by ``` --model-control-mode=explicit ```
- POLL : Load all model at run time and look for their changes all over the time. set time for looking for changes by setting ```--repository-poll-secs```. use this mode by --model-control-mode=poll ```

## Model configuration

all models has a configure file which i think in protobuf format in the repository. we should set platform for different category of models like pytorch, tensorflow or tensorrt. also we should set max batch size for inference and over that, we should set inputs and outputs of model.

```
  platform: "tensorrt_plan"
  max_batch_size: 8
  input [
    {
      name: "input0"
      data_type: TYPE_FP32
      dims: [ 16 ]
    },
    {
      name: "input1"
      data_type: TYPE_FP32
      dims: [ 16 ]
    }
  ]
  output [
    {
      name: "output0"
      data_type: TYPE_FP32
      dims: [ 16 ]
    }
  ]
```
### Auto generated model config
In some cases, we can start triton by ``` --strict-model-config=false ``` that model config file will be generated by triton automatically. by explicit github of trition "tensorRT, TensorFlow saved-model, and ONNX models do not require a model configuration file because Triton can derive all the required settings automatically. "

we can get config file by using ``` curl localhost:8000/v2/models/<model name>/config```

## Model Repository Extension
### Index
return information about models by calling ```POST v2/repository/index```
request and return object of it is looks like
```
$repository_index_request =
{
  "ready" : boolean 
}
```

```
$repository_index_response =
[
  {
    "name" : $string,
    "version" : $string #optional,
    "state" : $string,
    "reason" : $string
  },
  â€¦
]
```

### LOAD

We can easily load model by this command 
```
POST v2/repository/models/${MODEL_NAME}/load

```

### Unload

we can unload modles from trition by 

```
POST v2/repository/models/${MODEL_NAME}/unload

```

## Metrics
with trition, we could get Prometheus metrics for GPU and request statistics.
use ``` curl localhost:8002/metrics``` for getting metrics because triton provide them on port 8002.

## Client
next to this document, we provide a client code for trition. like seldon, it provides http client for sending request and getting response. we can use model_name in it to send requests to different models.

## Kubernetes serverless inferencing
InferenceService is the custom resource specify the predictor of type triton.

```
apiVersion: "serving.kubeflow.org/v1alpha2"
kind: "InferenceService"
metadata:
  name: "bert-large"
spec:
  default:
    transformer:
  	custom:
    	  container:
          name: kfserving-container
          image: gcr.io/kubeflow-ci/kfserving/bert-transformer:latest
      	    resources:
        	limits:
          	  cpu: "1"
          	  memory: 1Gi
      	    command:
        	- "python"
        	- "-m"
        	- "bert_transformer"
          env:
        	- name: STORAGE_URI
          	  value: "gs://kfserving-samples/models/triton/bert-transformer"
    predictor:
  	triton:
    	  resources:
      	    limits:
            cpu: "1"
            memory: 16Gi
            nvidia.com/gpu: 1
    	  storageUri: "gs://nv-enterprise/trtis_models/"
```

