{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Server with Minio but no Seldon\n",
    " [source](https://thenewstack.io/deploy-nvidia-triton-inference-server-with-minio-as-model-store/?fr=)\n",
    " use the former notebook to generate model variants\n",
    " ### single node triton server with load test\n",
    " 1. Loading to and from minio workflow (multiple models)\n",
    " 2. Getting models from [timm](https://timm.fast.ai/)\n",
    " 3. Making the node\n",
    " 4. Load and unload models operations\n",
    " 5. Monitoring\n",
    " 6. Naive load test\n",
    " 7. TODO Add language models from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('sudo umount -l ~/my_mounting_point')\n",
    "os.system('cc-cloudfuse mount ~/my_mounting_point')\n",
    " \n",
    "data_folder_path = '/home/cc/my_mounting_point/datasets'\n",
    "dataset_folder_path = os.path.join(\n",
    "    data_folder_path, 'ILSVRC/Data/DET/test'\n",
    ")\n",
    "classes_file_path = os.path.join(\n",
    "    data_folder_path, 'imagenet_classes.txt'\n",
    ")\n",
    " \n",
    "image_names = os.listdir(dataset_folder_path)\n",
    "image_names.sort()\n",
    "with open(classes_file_path) as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "def image_loader(folder_path, image_name):\n",
    "    image = Image.open(\n",
    "        os.path.join(folder_path, image_name))\n",
    "    # if there was a need to filter out only color images\n",
    "    # if image.mode == 'RGB':\n",
    "    #     pass\n",
    "    return image\n",
    "num_loaded_images = 4\n",
    "images = {\n",
    "    image_name: image_loader(\n",
    "        dataset_folder_path, image_name) for image_name in image_names[\n",
    "            :num_loaded_images]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    " \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")])\n",
    " \n",
    "batch = torch.stack(list(map(lambda a: transform(a), list(images.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 224, 224])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lion', 'studio_couch', 'harp', 'goldfish'], dtype='<U30')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'resnet50'\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "model.eval()\n",
    "torch_output = model(batch)\n",
    "torch_output = torch.nn.functional.softmax(torch_output, dim=1) * 100\n",
    "torch_output = torch_output.detach().numpy()\n",
    "torch_output = torch_output.argmax(axis=1)\n",
    "torch_class = np.array(classes)[torch_output]\n",
    "torch_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the onnx model\n",
    "import torch.onnx\n",
    " \n",
    "model_variant = 1\n",
    "model_dir = os.path.join(\n",
    "    'models',\n",
    "    model_name,\n",
    "    str(model_variant))\n",
    "model_path = os.path.join(model_dir, 'model.onnx')\n",
    "if 'models' not in os.listdir(\"./\"):\n",
    "    os.makedirs(model_dir)\n",
    "# Standard ImageNet input - 3 channels, 224x224,\n",
    "# values don't matter as we care about network structure.\n",
    "# But they can also be real inputs.\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "# Invoke export\n",
    "torch.onnx.export(\n",
    "    model, dummy_input,\n",
    "    model_path,\n",
    "    input_names = ['input'],\n",
    "    output_names = ['output'],\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                  'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/resnet50/config.pbtxt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lion', 'studio_couch', 'slot', 'goldfish'], dtype='<U30')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use onnx model\n",
    "import onnx\n",
    "import onnxruntime\n",
    " \n",
    "onnx_model = onnx.load(model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    " \n",
    "ort_session = onnxruntime.InferenceSession(\n",
    "    os.path.join(model_dir, \"model.onnx\"),\n",
    "    providers=['CPUExecutionProvider'])\n",
    "onnx_output = ort_session.run(None, {'input': batch.numpy()})\n",
    "onnx_output = torch.nn.functional.softmax(torch.tensor(onnx_output), dim=1)[0] * 100\n",
    "onnx_output = onnx_output.detach().numpy()\n",
    "onnx_output = onnx_output.argmax(axis=1)\n",
    "onnx_class = np.array(classes)[onnx_output]\n",
    "onnx_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000009vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# TODO find out why slightly different\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000009vscode-remote?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mall(onnx_output \u001b[39m==\u001b[39m torch_output)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000009vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(onnx_class)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO find out why slightly different\n",
    " \n",
    "assert np.all(onnx_output == torch_output)\n",
    "print(onnx_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/resnet50/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/resnet50/config.pbtxt\n",
    "name: \"resnet50\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size : 100\n",
    "input [\n",
    "  {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.05-py3: Pulling from nvidia/tritonserver\n",
      "Digest: sha256:a85daa2907f46e70b3782818a0331df62d9b4e0b1f15f1530b2a52c8c782d46d\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "d4be65a38ca32d0685d70b03a379071b3f2f666f3aab9173a6af868e34422ed3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VERSION='22.05'\n",
    "os.system(f\"docker pull nvcr.io/nvidia/tritonserver:{VERSION}-py3\")\n",
    "# add --gpus=<number of gpus> on gpu machines\n",
    "# add -d to run at background and going to the next cell\n",
    "os.system(\"docker run --rm -d -p8000:8000 -p8001:8001 -p8002:8002\"\n",
    "          f\" -v {os.getcwd()}/models:/models \"\n",
    "          f\"nvcr.io/nvidia/tritonserver:{VERSION}-py3\"\n",
    "          \" tritonserver --model-repository=/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Client Examples\n",
    " \n",
    " [examples](https://github.com/triton-inference-server/client/tree/main/src/python/examples)\n",
    " \n",
    " [grpc](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/grpc/__init__.py)\n",
    " \n",
    " [http](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/http/__init__.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Python Client Examples\n",
      " \n",
      " [examples](https://github.com/triton-inference-server/client/tree/main/src/python/examples)\n",
      " \n",
      " [grpc](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/grpc/__init__.py)\n",
      " \n",
      " [http](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/http/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "print(    \"### Python Client Examples\\n\",\n",
    "    \"\\n\",\n",
    "    \"[examples](https://github.com/triton-inference-server/client/tree/main/src/python/examples)\\n\",\n",
    "    \"\\n\",\n",
    "    \"[grpc](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/grpc/__init__.py)\\n\",\n",
    "    \"\\n\",\n",
    "    \"[http](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/http/__init__.py)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "try:\n",
    "    triton_client = httpclient.InferenceServerClient(\n",
    "        url='localhost:8000', verbose=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"context creation failed: \" + str(e))\n",
    "\n",
    "model_name = \"resnet50\"\n",
    "inputs = []\n",
    "inputs.append(\n",
    "    httpclient.InferInput(\n",
    "        name=\"input\", shape=batch.shape, datatype=\"FP32\")\n",
    ")\n",
    "inputs[0].set_data_from_numpy(batch.numpy(), binary_data=False)\n",
    " \n",
    "outputs = []\n",
    "outputs.append(httpclient.InferRequestedOutput(name=\"output\"))\n",
    " \n",
    "result = triton_client.infer(\n",
    "    model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "triton_client.close()\n",
    "triton_output = result.as_numpy('output')\n",
    "triton_output = torch.nn.functional.softmax(\n",
    "    torch.tensor(triton_output), dim=1) * 100\n",
    "triton_output = triton_output.detach().numpy()\n",
    "triton_output = triton_output.argmax(axis=1)\n",
    "triton_class = np.array(classes)[triton_output]\n",
    "triton_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    " \n",
    "try:\n",
    "    triton_client = grpcclient.InferenceServerClient(\n",
    "        url='localhost:8001', verbose=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"context creation failed: \" + str(e))\n",
    "model_name = \"resnet50\"\n",
    " \n",
    "inputs = []\n",
    "inputs.append(\n",
    "    grpcclient.InferInput(name=\"input\", shape=batch.shape, datatype=\"FP32\")\n",
    ")\n",
    "inputs[0].set_data_from_numpy(batch.numpy())\n",
    "outputs = []\n",
    "outputs.append(grpcclient.InferRequestedOutput(name=\"output\"))\n",
    "\n",
    "result = triton_client.infer(model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "triton_client.close()\n",
    "triton_output = result.as_numpy('output')\n",
    "triton_output = torch.nn.functional.softmax(\n",
    "    torch.tensor(triton_output), dim=1) * 100\n",
    "triton_output = triton_output.detach().numpy()\n",
    "triton_output = triton_output.argmax(axis=1)\n",
    "triton_class = np.array(classes)[triton_output]\n",
    "triton_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"resnet50\"\n",
      "platform: \"onnxruntime_onnx\"\n",
      "max_batch_size: 100\n",
      "input [\n",
      "  {\n",
      "    name: \"input\"\n",
      "    data_type: TYPE_FP32\n",
      "    format: FORMAT_NCHW\n",
      "    dims: [ 3, 224, 224 ]\n",
      "  }\n",
      "]\n",
      "output [\n",
      "  {\n",
      "    name: \"output\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [ 1000 ]\n",
      "  }\n",
      "]\n",
      "version_policy: { all { }}\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def config_builder(\n",
    "  name: str, platform: str, max_batch_size: int,\n",
    "  ):\n",
    "  config = (f\"name: \\\"{name}\\\"\\n\"\n",
    "            f\"platform: \\\"{platform}\\\"\\n\"\n",
    "            f\"max_batch_size: {str(max_batch_size)}\")\n",
    "  common_config=\"\"\"\n",
    "input [\n",
    "  {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "]\n",
    "version_policy: { all { }}\n",
    "  \"\"\"\n",
    "  return config + common_config\n",
    " \n",
    "print(config_builder('resnet50', 'onnxruntime_onnx', 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    " \n",
    "def generate_model_variants(model_name: str = 'resnet',\n",
    "    versions: list = ['18', '34', '101']):\n",
    "    # model name\n",
    "    timm_models = timm.list_models(model_name+'*', pretrained=True)\n",
    "    model_path = os.path.join(\n",
    "        'models',\n",
    "        model_name,\n",
    "    )\n",
    "    config_path = os.path.join(\n",
    "        model_path,\n",
    "        'config.pbtxt')\n",
    "    # if 'models' not in os.listdir(\"./\"):\n",
    "    os.makedirs(model_path)\n",
    "    config = config_builder(\n",
    "        name=model_name,\n",
    "        platform='onnxruntime_onnx',\n",
    "        max_batch_size=100)\n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(config)\n",
    "    for variant_id, model_variant in enumerate(versions):\n",
    "        model_full_name = model_name + model_variant\n",
    "        if not model_full_name in timm_models:\n",
    "            raise ValueError(\n",
    "                f\"Model {model_full_name} does not exist\"\n",
    "            )\n",
    "        model = timm.create_model(model_full_name, pretrained=True)\n",
    "        model.eval()\n",
    "        dummy_input = torch.randn(1, 3, 224, 224)\n",
    "        model_variant_dir = os.path.join(model_path, str(variant_id+1))\n",
    "        model_variant_path = os.path.join(model_variant_dir, 'model.onnx')\n",
    "        # if 'models' not in os.listdir(\"./\"):\n",
    "        os.makedirs(model_variant_dir)\n",
    "        torch.onnx.export(\n",
    "            model, dummy_input,\n",
    "            model_variant_path,\n",
    "            input_names = ['input'],\n",
    "            output_names = ['output'],\n",
    "            dynamic_axes={'input' : {0 : 'batch_size'},\n",
    "                          'output' : {0 : 'batch_size'}})\n",
    " \n",
    "def model_generator(\n",
    "    model_names: List[str],\n",
    "    versions: List[List[str]]):\n",
    "    assert len(model_names) == len(versions),\\\n",
    "        \"length modes list {} does not match versions list {}\".fromat(\n",
    "            len(model_names),\n",
    "            len(versions)\n",
    "        )\n",
    "    for model_name, version in zip(model_names, versions):\n",
    "        generate_model_variants(\n",
    "            model_name=model_name,\n",
    "            versions=version\n",
    "        )\n",
    " \n",
    "# read these from json/yamls build with a proper config builder\n",
    "model_generator(\n",
    "    model_names = ['resnet', 'xception'],\n",
    "    versions = [['18', '34', '101'], ['', '41', '65', '71']]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.05-py3: Pulling from nvidia/tritonserver\n",
      "Digest: sha256:a85daa2907f46e70b3782818a0331df62d9b4e0b1f15f1530b2a52c8c782d46d\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "3cdca3d3cde8c784ee730432627a571b01e65fe357866cda02b01895f874fe71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate models\n",
    "VERSION='22.05'\n",
    "os.system(f\"docker pull nvcr.io/nvidia/tritonserver:{VERSION}-py3\")\n",
    "# add --gpus=<number of gpus> on gpu machines\n",
    "# add -d to run at background and going to the next cell\n",
    "os.system(\"docker run --rm -d -p8000:8000 -p8001:8001 -p8002:8002\"\n",
    "          f\" -v {os.getcwd()}/models:/models \"\n",
    "          f\"nvcr.io/nvidia/tritonserver:{VERSION}-py3\"\n",
    "          \" tritonserver --model-repository=/models\")\n",
    "# print(\"docker run --rm -d -p8000:8000 -p8001:8001 -p8002:8002\"\n",
    "#       f\" -v {os.getcwd()}/models:/models \"\n",
    "#       f\"nvcr.io/nvidia/tritonserver:{VERSION}-py3\"\n",
    "#       \" tritonserver --strict-model-config=false --model-repository=/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to multi-models\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    " \n",
    " \n",
    "model_version = \"1\"\n",
    " \n",
    "try:\n",
    "    triton_client = httpclient.InferenceServerClient(\n",
    "        url='localhost:8000', verbose=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"context creation failed: \" + str(e))\n",
    "model_name = \"resnet\"\n",
    "\n",
    "inputs = []\n",
    "inputs.append(\n",
    "    httpclient.InferInput(name=\"input\", shape=batch[0:4].shape, datatype=\"FP32\")\n",
    ")\n",
    "inputs[0].set_data_from_numpy(batch[0:4].numpy(), binary_data=False)\n",
    " \n",
    "outputs = []\n",
    "outputs.append(httpclient.InferRequestedOutput(name=\"output\"))\n",
    "\n",
    "result = triton_client.infer(\n",
    "    model_name=model_name,\n",
    "    model_version=model_version, # different form the older model\n",
    "    inputs=inputs, outputs=outputs)\n",
    "triton_client.close()\n",
    "# result.get_response()\n",
    "triton_output = result.as_numpy('output')\n",
    "triton_output = torch.nn.functional.softmax(\n",
    "    torch.tensor(triton_output), dim=1) * 100\n",
    "triton_output = triton_output.detach().numpy()\n",
    "triton_output = triton_output.argmax(axis=1)\n",
    "triton_class = np.array(classes)[triton_output]\n",
    "print(triton_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cougar', 'comic_book', 'iron', 'goldfish'], dtype='<U30')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.05-py3: Pulling from nvidia/tritonserver\n",
      "Digest: sha256:a85daa2907f46e70b3782818a0331df62d9b4e0b1f15f1530b2a52c8c782d46d\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "3ef7f53fca88af23a4ff9aab2513e81bc8de7b8bee6c6f4c67e93822d6719b3e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate models\n",
    "VERSION='22.05'\n",
    "MODEL_MANAGEMENT = 'explicit'\n",
    "os.system(f\"docker pull nvcr.io/nvidia/tritonserver:{VERSION}-py3\")\n",
    "# add --gpus=<number of gpus> on gpu machines\n",
    "# add -d to run at background and going to the next cell\n",
    "os.system(\"docker run --rm -d -p8000:8000 -p8001:8001 -p8002:8002\"\n",
    "          f\" -v {os.getcwd()}/models:/models \"\n",
    "          f\"nvcr.io/nvidia/tritonserver:{VERSION}-py3\"\n",
    "          \" tritonserver --model-repository=/models\"\n",
    "          f\" --model-control-mode={MODEL_MANAGEMENT} --load-model=*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------active models--------------------\n",
      "\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '345'}>\n",
      "bytearray(b'[{\"name\":\"resnet\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"resnet\",\"version\":\"2\",\"state\":\"READY\"},{\"name\":\"resnet\",\"version\":\"3\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"2\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"3\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"4\",\"state\":\"READY\"}]')\n",
      "{'name': 'resnet', 'version': '1', 'state': 'READY'}\n",
      "{'name': 'resnet', 'version': '2', 'state': 'READY'}\n",
      "{'name': 'resnet', 'version': '3', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '1', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '2', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '3', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '4', 'state': 'READY'}\n",
      "--------------------unloading model: resnet--------------------\n",
      "\n",
      "POST /v2/repository/models/resnet/unload, headers None\n",
      "{\"parameters\":{\"unload_dependents\":false}}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'resnet'\n",
      "None\n",
      "--------------------active models after unloading--------------------\n",
      "\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '357'}>\n",
      "bytearray(b'[{\"name\":\"resnet\",\"version\":\"1\",\"state\":\"UNLOADING\"},{\"name\":\"resnet\",\"version\":\"2\",\"state\":\"UNLOADING\"},{\"name\":\"resnet\",\"version\":\"3\",\"state\":\"UNLOADING\"},{\"name\":\"xception\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"2\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"3\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"4\",\"state\":\"READY\"}]')\n",
      "{'name': 'resnet', 'version': '1', 'state': 'UNLOADING'}\n",
      "{'name': 'resnet', 'version': '2', 'state': 'UNLOADING'}\n",
      "{'name': 'resnet', 'version': '3', 'state': 'UNLOADING'}\n",
      "{'name': 'xception', 'version': '1', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '2', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '3', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '4', 'state': 'READY'}\n",
      "--------------------load model: resnet--------------------\n",
      "\n",
      "POST /v2/repository/models/resnet/load, headers None\n",
      "{}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'resnet'\n",
      "None\n",
      "--------------------active models after loading back--------------------\n",
      "\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '345'}>\n",
      "bytearray(b'[{\"name\":\"resnet\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"resnet\",\"version\":\"2\",\"state\":\"READY\"},{\"name\":\"resnet\",\"version\":\"3\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"2\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"3\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"4\",\"state\":\"READY\"}]')\n",
      "{'name': 'resnet', 'version': '1', 'state': 'READY'}\n",
      "{'name': 'resnet', 'version': '2', 'state': 'READY'}\n",
      "{'name': 'resnet', 'version': '3', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '1', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '2', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '3', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '4', 'state': 'READY'}\n"
     ]
    }
   ],
   "source": [
    "# see \n",
    "# https://github.com/triton-inference-server/client/blob/main/src/python/examples/simple_http_model_control.py\n",
    "# https://github.com/triton-inference-server/server/blob/main/docs/model_management.md\n",
    "# https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md\n",
    " \n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    " \n",
    " \n",
    "try:\n",
    "    triton_client = httpclient.InferenceServerClient(\n",
    "        url='localhost:8000', verbose=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "     print(\"context creation failed: \" + str(e))\n",
    "model_name = \"resnet\"\n",
    " \n",
    " \n",
    "print(20*'-' + 'active models' + 20*'-' + '\\n')\n",
    "print(*triton_client.get_model_repository_index(), sep='\\n')\n",
    " \n",
    "print(20*'-' + f'unloading model: {model_name}' + 20*'-' + '\\n')\n",
    "print(triton_client.unload_model(model_name))\n",
    " \n",
    "print(20*'-' + 'active models after unloading' + 20*'-' + '\\n')\n",
    "print(*triton_client.get_model_repository_index(), sep='\\n')\n",
    " \n",
    "print(20*'-' + f'load model: {model_name}' + 20*'-' + '\\n')\n",
    "print(triton_client.load_model(model_name))\n",
    " \n",
    "print(20*'-' + 'active models after loading back' + 20*'-' + '\\n')\n",
    "print(*triton_client.get_model_repository_index(), sep='\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32;1mBucket created successfully `minio/minio-seldon`.\u001b[0m\n",
      "...fig.pbtxt:  800.38 MiB / 800.38 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 183.41 MiB/s 4s\u001b[0m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "# copy files\n",
    "!mc mb minio/minio-seldon -p\n",
    "!mc cp -r ./models minio/minio-seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting secret.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile secret.yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: seldon-init-container-secret\n",
    "type: Opaque\n",
    "stringData:\n",
    "  RCLONE_CONFIG_S3_TYPE: s3\n",
    "  RCLONE_CONFIG_S3_PROVIDER: minio\n",
    "  RCLONE_CONFIG_S3_ENV_AUTH: \"false\"\n",
    "  RCLONE_CONFIG_S3_ACCESS_KEY_ID: minioadmin\n",
    "  RCLONE_CONFIG_S3_SECRET_ACCESS_KEY: minioadmin\n",
    "  RCLONE_CONFIG_S3_ENDPOINT: http://minio.minio-system.svc.cluster.local:9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting seldon-triton.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile seldon-triton.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: resnet\n",
    "spec:\n",
    "  name: default\n",
    "  predictors:\n",
    "  - graph:\n",
    "      implementation: TRITON_SERVER\n",
    "      logger:\n",
    "        mode: all\n",
    "      modelUri: s3://minio-seldon/models\n",
    "      envSecretRefName: seldon-init-container-secret\n",
    "      name: resnet # This should have the same name as the model inside\n",
    "      type: MODEL\n",
    "    name: default\n",
    "    replicas: 1\n",
    "  protocol: kfserving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/seldon-init-container-secret configured\n",
      "seldondeployment.machinelearning.seldon.io/resnet unchanged\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f secret.yaml -n default\n",
    "!kubectl apply -f seldon-triton.yaml -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://localhost:32000/seldon/default/resnet/v2/models/resnet | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cougar' 'brassiere' 'prayer_rug' 'goldfish']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " # from subprocess import PIPE, Popen, run\n",
    "import requests\n",
    " \n",
    "import numpy as np\n",
    " \n",
    " \n",
    "URL = \"http://localhost:32000/seldon/default/resnet\"\n",
    " \n",
    " \n",
    "def predict(data):\n",
    "    data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"input\",\n",
    "                \"data\": data.tolist(),\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"shape\": data.shape,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    " \n",
    "    r = requests.post(f\"{URL}/v2/models/resnet/infer\", json=data)\n",
    "    predictions = np.array(r.json()[\"outputs\"][0][\"data\"]).reshape(\n",
    "        r.json()[\"outputs\"][0][\"shape\"]\n",
    "    )\n",
    "    output = [np.argmax(x) for x in predictions]\n",
    "    return output\n",
    "triton_seldon_output = predict(batch.numpy())\n",
    "triton_seldon_classes = np.array(classes)[triton_seldon_output]\n",
    "print(triton_seldon_classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('central')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2465c4f56298bc06dbdad3e7519856d346ec0e9edf6ba2c905f0af711583810e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
