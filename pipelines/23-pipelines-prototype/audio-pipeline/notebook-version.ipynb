{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/miniconda3/envs/central/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.973744809627533}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment  = pipeline(\n",
    "    task=\"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sentiment(\"mamooli weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The task does not provide any default models for options ('es', 'en')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m translator  \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtranslation_es_to_en\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/nlp/notebook-version.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m translator(\u001b[39m\"\u001b[39m\u001b[39mHello, how are you?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/transformers/pipelines/__init__.py:619\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39m# Use default model/config/tokenizer for the task if no model is provided\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[39m# At that point framework might still be undetermined\u001b[39;00m\n\u001b[0;32m--> 619\u001b[0m     model, default_revision \u001b[39m=\u001b[39m get_default_model_and_revision(targeted_task, framework, task_options)\n\u001b[1;32m    620\u001b[0m     revision \u001b[39m=\u001b[39m revision \u001b[39mif\u001b[39;00m revision \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m default_revision\n\u001b[1;32m    621\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    622\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo model was supplied, defaulted to \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m and revision\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    623\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    624\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a pipeline without specifying a model name and revision in production is not recommended.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/transformers/pipelines/base.py:374\u001b[0m, in \u001b[0;36mget_default_model_and_revision\u001b[0;34m(targeted_task, framework, task_options)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mif\u001b[39;00m task_options:\n\u001b[1;32m    373\u001b[0m     \u001b[39mif\u001b[39;00m task_options \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m defaults:\n\u001b[0;32m--> 374\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe task does not provide any default models for options \u001b[39m\u001b[39m{\u001b[39;00mtask_options\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    375\u001b[0m     default_models \u001b[39m=\u001b[39m defaults[task_options][\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    376\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m defaults:\n",
      "\u001b[0;31mValueError\u001b[0m: The task does not provide any default models for options ('es', 'en')"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "translator  = pipeline(\"translation_es_to_en\")\n",
    "translator(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading config.json: 100%|██████████| 1.76k/1.76k [00:00<00:00, 962kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.14G/1.14G [00:12<00:00, 99.7MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 14.7kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 878k/878k [00:00<00:00, 6.86MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 446k/446k [00:00<00:00, 4.90MB/s]\n",
      "Your max_length is set to 142, but you input_length is only 94. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \" If hearings or trials cannot take place because there are no barristers present to represent defendants, there won't be any trials in which criminals are sent to prison and those who are innocent are acquitted . Victims, like defendants, will be left in limbo, unsure when they will see justice . Tentative plans to broker a deal behind the scenes by bringing forward payments have failed .\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summerize  = pipeline(\"summarization\")\n",
    "summerize(\"If hearings or trials cannot take place because there are no barristers present to represent defendants, there won't be any trials in which criminals are sent to prison and those who are innocent are acquitted. Victims, like defendants, will be left in limbo, unsure when they will see justice. Tentative plans to broker a deal behind the scenes by bringing forward payments have failed - partly because there is simply no trust between the profession and ministers who wont meet them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'en', 'score': 0.9999854564666748}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "translator  = pipeline(task=\"text-classification\", model=\"dinalzein/xlm-roberta-base-finetuned-language-identification\")\n",
    "translator(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"hf-internal-testing/librispeech_asr_demo\",\n",
    "    \"clean\",\n",
    "    split=\"validation\")\n",
    "\n",
    "inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/miniconda3/envs/central/lib/python3.8/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:561: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input_lengths = (input_lengths - 1) // 2 + 1\n",
      "/home/cc/miniconda3/envs/central/lib/python3.8/site-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "translator  = pipeline(task=\"automatic-speech-recognition\", model=\"facebook/s2t-small-librispeech-asr\")\n",
    "translator(ds[0][\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "translator  = pipeline(task=\"automatic-speech-recognition\", model=\"facebook/s2t-small-librispeech-asr\")\n",
    "translator(ds[0][\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "       0.0010376 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\n",
    "    \"hf-internal-testing/librispeech_asr_demo\",\n",
    "    \"clean\",\n",
    "    split=\"validation\")\n",
    "\n",
    "inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n",
    "generated_ids = model.generate(inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/miniconda3/envs/central/lib/python3.8/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:561: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input_lengths = (input_lengths - 1) // 2 + 1\n",
      "/home/cc/miniconda3/envs/central/lib/python3.8/site-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'returned the master and the master of the town'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset, Audio, Dataset\n",
    "\n",
    "path = \"/home/cc/infernece-pipeline-joint-optimization/pipelines/21-pipelines-prototype/audio-pipeline/seldon-core-version/sample-dataset.mp3\"\n",
    "translator  = pipeline(task=\"automatic-speech-recognition\", model=\"facebook/s2t-small-librispeech-asr\")\n",
    "audio_dataset = Dataset.from_dict({\"audio\": [path]}).cast_column(\"audio\", Audio())\n",
    "audio_dataset[0]\n",
    "\n",
    "\n",
    "translator(audio_dataset[0][\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2465c4f56298bc06dbdad3e7519856d346ec0e9edf6ba2c905f0af711583810e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('central')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
