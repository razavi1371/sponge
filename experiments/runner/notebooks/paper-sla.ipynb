{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video pipeline with Yolo + Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import PrettyPrinter\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "\n",
    "pp = PrettyPrinter(indent=4)\n",
    "from barazmoon.twitter import twitter_workload_generator\n",
    "\n",
    "# get an absolute path to the directory that contains parent files\n",
    "__file__ = globals()[\"_dh\"][0]\n",
    "project_dir = __file__ = globals()[\"_dh\"][0]\n",
    "sys.path.append(os.path.normpath(os.path.join(project_dir, \"..\", \"..\", \"..\")))\n",
    "\n",
    "from experiments.utils.constants import FINAL_RESULTS_PATH\n",
    "from experiments.utils.parser import AdaptationParser\n",
    "from experiments.utils.drawing import draw_temporal, draw_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaserieses = [21, 21, 21, 21, 21]\n",
    "serieses = [1, 21, 41, 61, 81]\n",
    "\n",
    "series_names = None\n",
    "# series_names = {\n",
    "#     # 1: \"IPA-accuracy\",\n",
    "#     2: \"IPA\",\n",
    "#     # 3: \"IPA-cost\",\n",
    "#     4: \"FA2-high\",\n",
    "#     # 5: \"FA2-high\",\n",
    "#     # 6: \"RIM-low\",\n",
    "#     7: \"RIM-high\",\n",
    "# }\n",
    "factors = {\n",
    "    1: 2/3,\n",
    "    21: 2,\n",
    "    41: 2,\n",
    "    61: 1,\n",
    "    81: 2/3\n",
    "}\n",
    "series_paths = {\n",
    "    series: os.path.join(\n",
    "        FINAL_RESULTS_PATH, \"metaseries\", str(metaseries), \"series\", str(series)\n",
    "    )\n",
    "    for series, metaseries in zip(serieses, metaserieses)\n",
    "}\n",
    "\n",
    "loaders = {\n",
    "    series: AdaptationParser(\n",
    "        series_path=series_path, model_name=\"video\", type_of=\"router_pipeline\"\n",
    "    )\n",
    "    for series, series_path in series_paths.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 10, 21: 20, 41: 20, 61: 5, 81: 20}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_modes = {}\n",
    "configs = {}\n",
    "drop_limit = {}\n",
    "for series, loader in loaders.items():\n",
    "    configs_exp = loader.load_configs()\n",
    "    config = configs_exp[\"0.yaml\"]\n",
    "    configs[series] = config\n",
    "    simulation_modes[series] = config[\"simulation_mode\"]\n",
    "    drop_limit[series] = config[\"drop_limit\"]\n",
    "drop_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sent workload\n",
    "sent_loads = {}\n",
    "for series, config in configs.items():\n",
    "    workload_type = config[\"workload_type\"]\n",
    "    workload_config = config[\"workload_config\"][0]\n",
    "    start = workload_config[\"start\"]\n",
    "    end = workload_config[\"end\"]\n",
    "    damping_factor = workload_config[\"damping_factor\"]\n",
    "    sent_loads[series] = twitter_workload_generator(\n",
    "        days=f\"{start}-{end}\", damping_factor=damping_factor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation_logs = dict(\n",
    "    map(lambda l: (l[0], l[1].load_adaptation_log()), loaders.items())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 6.897365409135817, 21: 9.235554277896881, 41: 10.132449761033056, 61: 3.851757884025572, 81: 17.619139596819878}\n",
      "{1: {'crop': 4.620585805177688, 'classification': 2.2767796039581296}, 21: {'audio': 8.344102382659912, 'nlp-qa': 0.8914518952369689}, 41: {'audio': 9.043847739696501, 'nlp-sent': 1.0886020213365553}, 61: {'nlp-sum': 2.527679383754729, 'nlp-qa': 1.324078500270843}, 81: {'nlp-li': 0.9724401533603668, 'nlp-trans': 12.855180367827415, 'nlp-sum': 3.7915190756320936}}\n"
     ]
    }
   ],
   "source": [
    "slas = {}\n",
    "stage_wise_slas = {}\n",
    "series_changes = {}\n",
    "for series in serieses:\n",
    "    series_changes[series] = loaders[series].series_changes(\n",
    "        adaptation_log=adaptation_logs[series]\n",
    "    )\n",
    "    slas[series] = series_changes[series][\"sla\"] / factors[series]\n",
    "    stage_wise_slas[series] = dict(\n",
    "        map(\n",
    "            lambda l: (l[0], l[1] / factors[series]),\n",
    "            series_changes[series][\"stage_wise_slas\"].items(),\n",
    "        )\n",
    "    )\n",
    "print(slas)\n",
    "print(stage_wise_slas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'sla': 6.897365409135817,\n",
       "  'stage_wise_slas': {'crop': 4.620585805177688,\n",
       "   'classification': 2.2767796039581296},\n",
       "  'drop_limit': 10},\n",
       " 21: {'sla': 9.235554277896881,\n",
       "  'stage_wise_slas': {'audio': 8.344102382659912,\n",
       "   'nlp-qa': 0.8914518952369689},\n",
       "  'drop_limit': 20},\n",
       " 41: {'sla': 10.132449761033056,\n",
       "  'stage_wise_slas': {'audio': 9.043847739696501,\n",
       "   'nlp-sent': 1.0886020213365553},\n",
       "  'drop_limit': 20},\n",
       " 61: {'sla': 3.851757884025572,\n",
       "  'stage_wise_slas': {'nlp-sum': 2.527679383754729,\n",
       "   'nlp-qa': 1.324078500270843},\n",
       "  'drop_limit': 5},\n",
       " 81: {'sla': 17.619139596819878,\n",
       "  'stage_wise_slas': {'nlp-li': 0.9724401533603668,\n",
       "   'nlp-trans': 12.855180367827415,\n",
       "   'nlp-sum': 3.7915190756320936},\n",
       "  'drop_limit': 20}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sla_and_drop = {}\n",
    "for series in serieses:\n",
    "    sla_and_drop[series] = {}\n",
    "    sla_and_drop[series][\"sla\"] = slas[series]\n",
    "    sla_and_drop[series][\"stage_wise_slas\"] = stage_wise_slas[series]\n",
    "    sla_and_drop[series][\"drop_limit\"] = drop_limit[series]\n",
    "sla_and_drop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Latencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maeseured p99 Latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout_per_second = {}\n",
    "per_second_results = {}\n",
    "for series in serieses:\n",
    "    if not simulation_modes[series]:\n",
    "        timeout_per_second[series], per_second_results[series] = loaders[\n",
    "            series\n",
    "        ].per_second_result_processing()\n",
    "    else:\n",
    "        timeout_per_second[series], per_second_results[series] = None, None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if not all(simulation_modes.values()):\n",
    "    ylabel = \"% SLA Violations\"\n",
    "    xlabel = \"Experiments\"\n",
    "    timeout_dics = {}\n",
    "    for series in serieses:\n",
    "        # print(50 * \"-\" + f\" {series} \" + 50 * \"-\")\n",
    "        if not simulation_modes[series]:\n",
    "            timeout_dics[series] = {\n",
    "                \"\": (\n",
    "                    np.array(timeout_per_second[series]) / sum(sent_loads[series])\n",
    "                ).tolist()\n",
    "            }\n",
    "            # draw_temporal(timeout_dics[series])\n",
    "            print(f\"{sum(timeout_per_second[series])} out of {sum(sent_loads[series])}\")\n",
    "    draw_temporal(timeout_dics, multiple_experiments=True, ylabel=ylabel)\n",
    "    draw_cumulative(\n",
    "        timeout_dics,\n",
    "        multiple_experiments=True,\n",
    "        ylabel=ylabel,\n",
    "        series_names=series_names,\n",
    "        xlabel=xlabel,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "central",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2465c4f56298bc06dbdad3e7519856d346ec0e9edf6ba2c905f0af711583810e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
